{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChNQ4XjtRkq"
      },
      "source": [
        "### Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLsUmByDdUdb",
        "outputId": "48127181-7d8f-406c-ad6c-7bfc0f9b6526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "### NLP Libraries\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import  word_tokenize \n",
        "import nltk\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, classification_report\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "stpwords = set(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.options.display.max_colwidth = 500\n",
        "\n",
        "\n",
        "### Importing TensorFlow Libraries\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten, Dropout, Bidirectional, Multiply, Lambda\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "## Importing Word2Vec Embeddings\n",
        "\n",
        "import gensim\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from gensim.models import KeyedVectors\n",
        "filename = '/content/drive/My Drive/GoogleNews-vectors-negative300.bin'\n",
        "w2v_dictionary = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "\n",
        "# if glove file is on your google drive and you are running the notebook on colab, run this code below, else ignore\n",
        "\n",
        "\n",
        "### Importing Glove Embeddings\n",
        "#path = '/content/drive/My Drive/glove.6B.200d.txt'\n",
        "### Loading Glove vectors\n",
        "  # g_dictionary = {}\n",
        "  # with open(path) as file:\n",
        "  #   for each_line in file:\n",
        "  #       words_in_line, coeff_cients = each_line.split(maxsplit=1)\n",
        "  #       coeff_cients = np.array(coeff_cients.split(),dtype = float)\n",
        "  #       glove_dictionary[words_in_line] = coeff_cients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "biOn-3Jt61oT"
      },
      "outputs": [],
      "source": [
        "def clean_message(text):\n",
        "    '''Function to clean and preprocess the input text'''\n",
        "\n",
        "    text = re.sub(\"'\", \"\", text) # to avoid removing contractions in english\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    text = re.sub(r'\\'s', '', text)\n",
        "    text = re.sub(r'<br />', '', text)\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub('[()!?]', ' ', text)\n",
        "    text = re.sub('\\[.*?\\]',' ', text)\n",
        "    text = re.sub(\"[^a-z0-9]\",\" \", text)\n",
        "    text = text.lower()\n",
        "    text_links_removed = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed) if word not in stpwords])\n",
        "    #text_cleaned = \" \".join([word for word in re.split('\\W+', text_links_removed)])\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in re.split('\\W+', text_cleaned)])\n",
        "    text = text.strip()\n",
        "    return text\n",
        "        \n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "def exponent_neg_euclidean_distance(left, right):\n",
        "    return K.exp(-(K.sqrt(K.sum(K.square(left-right), axis=1, keepdims=True))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUdE5mPdtVc0"
      },
      "source": [
        "### Input Questions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def load_pickle_file(path):\n",
        "  with open(path, 'rb') as fp:\n",
        "    embedding_df = pickle.load(fp)\n",
        "    fp.close()\n",
        "  \n",
        "\n",
        "    return embedding_df\n",
        "\n",
        "pickle_path = '/content/drive/My Drive/quora_question_pairs_w2v_lstm.pkl'\n",
        "model = load_pickle_file(pickle_path)\n",
        "pickle_path = '/content/drive/My Drive/quora_question_pairs_w2v_tokenizer.pkl'\n",
        "tokenizer = load_pickle_file(pickle_path)\n",
        "pickle_path = '/content/drive/My Drive/quora_question_pairs_w2v_embedding_matrix.pkl'\n",
        "embedding_matrix = load_pickle_file(pickle_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BraQWFZ3B9aH",
        "outputId": "f758a696-ee48-4885-8070-145b13101a60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "variables.h5                                   2023-03-28 02:16:48    545438344\n",
            "config.json                                    2023-03-28 02:16:46         5101\n",
            "metadata.json                                  2023-03-28 02:16:46           64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......embedding\n",
            ".........vars\n",
            "............0\n",
            "......embedding_1\n",
            ".........vars\n",
            "............0\n",
            "......input_layer\n",
            ".........vars\n",
            "......input_layer_1\n",
            ".........vars\n",
            "......lambda\n",
            ".........vars\n",
            "......lstm\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "......lstm_1\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "...metrics\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......mean_metric_wrapper\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......precision\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......recall\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........13\n",
            ".........14\n",
            ".........15\n",
            ".........16\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Input Questions\n",
        "\n",
        "q1 = 'What is the best/most memorable thing youve ever eaten and why?'\n",
        "q2 = 'What is the most delicious dish youve ever eaten and why?'"
      ],
      "metadata": {
        "id": "81QlfBUyBQE0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qEoMcMH0scID"
      },
      "outputs": [],
      "source": [
        "### Cleaning Question 1 and Question 2\n",
        "q1 = clean_message(q1)\n",
        "q2 = clean_message(q2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Tokenizing the questions\n",
        "q1 = tokenizer.texts_to_sequences(q1)\n",
        "q2 = tokenizer.texts_to_sequences(q2)\n"
      ],
      "metadata": {
        "id": "vAkwp_odEuPg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Padding the input questions\n",
        "\n",
        "q1 = pad_sequences(q1, maxlen = 681)\n",
        "q2 = pad_sequences(q2, maxlen = 681)"
      ],
      "metadata": {
        "id": "bgJVTuWsFFo5"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Predicting if the given questions are similar using the selected model\n",
        "\n",
        "preds = model.predict([q1,q2])\n",
        "\n",
        "if preds > 0.5:\n",
        "  print('The questions are similar')\n",
        "else:\n",
        "  print(\"The questions are not similar\")"
      ],
      "metadata": {
        "id": "Fu8nBl1eJn0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uscxrpltAenc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}