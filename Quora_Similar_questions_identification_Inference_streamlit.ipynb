{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BasilKVarghese/QuoraSimilarQuestionIndentification/blob/main/Quora_Similar_questions_identification_Inference_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChNQ4XjtRkq"
      },
      "source": [
        "### Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGdXrSf7DwyF",
        "outputId": "2b971af9-6253-4558-86f7-085d38386dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import streamlit as st\n",
        "except:\n",
        "  !pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMqx2jxIjlGD",
        "outputId": "bb207ce9-7d86-411b-c7e1-26ab3d100316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.21.0-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.5.3)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.4.0)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (23.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.22.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (13.3.3)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (5.3.0)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.2)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit) (2.14.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2023.3)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=e9d9da496d52e3cccfd18c8d553946a6374c893a1ea1782e595355723ace8211\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, pympler, blinker, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed blinker-1.6.2 gitdb-4.0.10 gitpython-3.1.31 pydeck-0.8.0 pympler-1.0.1 smmap-5.0.0 streamlit-1.21.0 validators-0.20.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5quf12UAEkvz",
        "outputId": "fa3bddec-bde5-4ec0-ba16-12a3a88e695f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: streamlit\n",
            "Version: 1.21.0\n",
            "Summary: The fastest way to build data apps in Python\n",
            "Home-page: https://streamlit.io\n",
            "Author: Snowflake Inc\n",
            "Author-email: hello@streamlit.io\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: altair, blinker, cachetools, click, gitpython, importlib-metadata, numpy, packaging, pandas, pillow, protobuf, pyarrow, pydeck, pympler, python-dateutil, requests, rich, toml, tornado, typing-extensions, tzlocal, validators, watchdog\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "\n",
        "# import re\n",
        "# import nltk\n",
        "# import pickle\n",
        "# import keras.backend as K\n",
        "\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# from nltk.corpus import stopwords\n",
        "# stpwords = set(stopwords.words('english'))\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# def exponent_neg_manhattan_distance(left, right):\n",
        "#     return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "# def exponent_neg_manhattan_distance_tf(left, right):\n",
        "#     return tf.norm([left,right])\n",
        "\n",
        "# def exponent_neg_euclidean_distance(left, right):\n",
        "#     return tf.exp(-tf.sqrt(tf.reduce_sum(tf.square(left - right), axis=1, keepdims=True)))\n",
        "\n",
        "# def exponent_neg_cosine_distance(vector1, vector2):\n",
        "#   dot_product = tf.reduce_sum(tf.multiply(vector1, vector2), axis=1, keepdims=True)\n",
        "#   magnitude = tf.multiply(tf.sqrt(tf.reduce_sum(tf.square(vector1), axis=1, keepdims=True)), \n",
        "#                             tf.sqrt(tf.reduce_sum(tf.square(vector2), axis=1, keepdims=True)))\n",
        "#   return tf.exp(-(1-tf.divide(dot_product, magnitude)))\n",
        "\n",
        "# # Load the pre-trained model\n",
        "# with open('/content/drive/My Drive/quora_question_pairs_w2v_lstm_2.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n",
        "\n",
        "# # Load the pre-trained model\n",
        "# with open('/content/drive/My Drive/quora_question_pairs_w2v_tokenizer_2.pkl', 'rb') as f:\n",
        "#     tokenizer = pickle.load(f)\n",
        "\n",
        "# def clean_message(text):\n",
        "#     '''Function to clean and preprocess the input text'''\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(\"'\", \"\", text) # to avoid removing contractions in english\n",
        "#     text = re.sub(r'\\$\\w*', '', text)\n",
        "#     text = re.sub(r'\\'s', '', text)\n",
        "#     text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
        "#     text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "#     text = re.sub('[()!?,:\"]', '', text)\n",
        "#     text = re.sub('the us','usa',text)\n",
        "#     text = re.sub(\"[^a-z0-9]\",\" \", text)\n",
        "#     text = \" \".join([word for word in text.split(' ') if word not in stpwords])\n",
        "#     text = re.sub(' +', ' ',text)\n",
        "#     text = text.strip()\n",
        "#     return text\n",
        "\n",
        "\n",
        "# data_org = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone Quora Question Pairs/Quora_unique_questions.csv\")\n",
        "# # Define a function to predict if two questions are similar or not\n",
        "# def predict_similarity(question1, question2):\n",
        "#     max_sequence_length = 648\n",
        "#     # Preprocess the questions\n",
        "#     question1 = clean_message(question1)\n",
        "#     question2 = clean_message(question2)\n",
        "\n",
        "#     # Convert the questions to sequences of integers\n",
        "#     question1_sequence = tokenizer.texts_to_sequences([question1])\n",
        "#     question2_sequence = tokenizer.texts_to_sequences([question2])\n",
        "\n",
        "#     # Pad the sequences to the same length\n",
        "#     question1_sequence = pad_sequences(question1_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "#     question2_sequence = pad_sequences(question2_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "#     # Make the prediction\n",
        "#     prediction = model.predict([question1_sequence, question2_sequence])[0][0]\n",
        "\n",
        "#     # Return the result\n",
        "#     return prediction\n",
        "\n",
        "# def predict_similarity_batch(question1, question2):\n",
        "#     max_sequence_length = 648\n",
        "#     # Preprocess the questions\n",
        "#     question1 = question1.apply(clean_message)\n",
        "#     #print(question1)\n",
        "#     question2 = question2.apply(clean_message)\n",
        "#     #print(question2)\n",
        "\n",
        "#     # Convert the questions to sequences of integers\n",
        "#     question1_sequence = tokenizer.texts_to_sequences(question1)\n",
        "#     question2_sequence = tokenizer.texts_to_sequences(question2)\n",
        "\n",
        "#     # Pad the sequences to the same length\n",
        "#     question1_sequence = pad_sequences(question1_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "#     question2_sequence = pad_sequences(question2_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "#     # Make the prediction\n",
        "#     prediction = model.predict([question1_sequence, question2_sequence],batch_size=1024)#[0][0]\n",
        "\n",
        "#     # Return the result\n",
        "#     return prediction\n",
        "\n",
        "\n",
        "# def get_similar_questions(query_question,data):\n",
        "#   #data_org = data.sample(100000)\n",
        "#   data_org = data.copy()\n",
        "#   #query_question = 'How can i invest in Share market in India?'\n",
        "#   batch_size = 50000\n",
        "#   result_size = 3\n",
        "#   prev = 0\n",
        "#   result_df = pd.DataFrame()\n",
        "#   for i in np.arange(batch_size,len(data_org),batch_size):\n",
        "#     #print(\"i= \",i)\n",
        "#     #print(\"prev= \",prev)\n",
        "#     data_batch = data_org.iloc[prev:i,:]\n",
        "#     prev = i\n",
        "\n",
        "#     question2 = data_batch['question']\n",
        "#     question1 = pd.Series([query_question] * len(question2))\n",
        "    \n",
        "#     #print(\"question1 = \",question1.shape)\n",
        "#     #print(\"question2 = \",question2.shape)\n",
        "\n",
        "#     results = predict_similarity_batch(question1, question2)\n",
        "#     results = list(results.reshape(batch_size))\n",
        "#     #print(\"results = \",len(results))\n",
        "#     df = pd.DataFrame({'question1':list(question1), 'question2':list(question2),'scores':results})\n",
        "#     df = df[df['scores']>=0.5]\n",
        "#     result_df = pd.concat([df,result_df],axis = 0)\n",
        "#     result_len = len(result_df)\n",
        "#     print(result_len)\n",
        "#     if result_len >= result_size:\n",
        "#       break;\n",
        "#   result_df = result_df.sort_values(by = 'scores',ascending = False)\n",
        "#   result_df.columns = ['Query Question','Similar Questions','Scores']\n",
        "#   result_df = result_df['Similar Questions'].reset_index(drop = True)\n",
        "  \n",
        "#   return pd.Series(result_df)\n",
        "\n",
        "# q = 'How to Invest in Share Market in India?'\n",
        "# data_org = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone Quora Question Pairs/Quora_unique_questions.csv\")\n",
        "# result = get_similar_questions(q,data_org)\n",
        "# result.head()"
      ],
      "metadata": {
        "id": "Pm-XL1JBuvpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "model_pickle_path = '/content/drive/My Drive/quora_question_pairs_w2v_lstm_2.pkl'\n",
        "model_tokenizer_path = '/content/drive/My Drive/quora_question_pairs_w2v_tokenizer_2.pkl'\n",
        "max_sequence_length = 679\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import keras.backend as K\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "stpwords = set(stopwords.words('english'))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "def exponent_neg_manhattan_distance_tf(left, right):\n",
        "    return tf.norm([left,right])\n",
        "\n",
        "def exponent_neg_euclidean_distance(left, right):\n",
        "    return tf.exp(-tf.sqrt(tf.reduce_sum(tf.square(left - right), axis=1, keepdims=True)))\n",
        "\n",
        "def exponent_neg_cosine_distance(vector1, vector2):\n",
        "  dot_product = tf.reduce_sum(tf.multiply(vector1, vector2), axis=1, keepdims=True)\n",
        "  magnitude = tf.multiply(tf.sqrt(tf.reduce_sum(tf.square(vector1), axis=1, keepdims=True)), \n",
        "                            tf.sqrt(tf.reduce_sum(tf.square(vector2), axis=1, keepdims=True)))\n",
        "  return tf.exp(-(1-tf.divide(dot_product, magnitude)))\n",
        "\n",
        "# Load the pre-trained model\n",
        "with open(model_pickle_path, 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Load the pre-trained model\n",
        "with open(model_tokenizer_path, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "def clean_message(text):\n",
        "    '''Function to clean and preprocess the input text'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"'\", \"\", text) # to avoid removing contractions in english\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    text = re.sub(r'\\'s', '', text)\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub('[()!?,:\"]', '', text)\n",
        "    text = re.sub('the us','usa',text)\n",
        "    text = re.sub(\"[^a-z0-9]\",\" \", text)\n",
        "    text = \" \".join([word for word in text.split(' ') if word not in stpwords])\n",
        "    text = re.sub(' +', ' ',text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "data_org = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Capstone Quora Question Pairs/Quora_unique_questions.csv\")\n",
        "# Define a function to predict if two questions are similar or not\n",
        "def predict_similarity(question1, question2):\n",
        "    #max_sequence_length = 648\n",
        "    # Preprocess the questions\n",
        "    question1 = clean_message(question1)\n",
        "    question2 = clean_message(question2)\n",
        "\n",
        "    # Convert the questions to sequences of integers\n",
        "    question1_sequence = tokenizer.texts_to_sequences([question1])\n",
        "    question2_sequence = tokenizer.texts_to_sequences([question2])\n",
        "\n",
        "    # Pad the sequences to the same length\n",
        "    question1_sequence = pad_sequences(question1_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "    question2_sequence = pad_sequences(question2_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict([question1_sequence, question2_sequence])[0][0]\n",
        "\n",
        "    # Return the result\n",
        "    return prediction\n",
        "\n",
        "def predict_similarity_batch(question1, question2):\n",
        "    #max_sequence_length = 648\n",
        "    # Preprocess the questions\n",
        "    question1 = question1.apply(clean_message)\n",
        "    #print(question1)\n",
        "    question2 = question2.apply(clean_message)\n",
        "    #print(question2)\n",
        "\n",
        "    # Convert the questions to sequences of integers\n",
        "    question1_sequence = tokenizer.texts_to_sequences(question1)\n",
        "    question2_sequence = tokenizer.texts_to_sequences(question2)\n",
        "\n",
        "    # Pad the sequences to the same length\n",
        "    question1_sequence = pad_sequences(question1_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "    question2_sequence = pad_sequences(question2_sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict([question1_sequence, question2_sequence],batch_size=512)#[0][0]\n",
        "\n",
        "    # Return the result\n",
        "    return prediction\n",
        "\n",
        "def get_similar_questions(query_question,data_org):\n",
        "  #data_org = data_org.sample(100000)\n",
        "  #data_org = data.copy()\n",
        "  #query_question = 'How can i invest in Share market in India?'\n",
        "  batch_size = 50000\n",
        "  result_size = 3\n",
        "  prev = 0\n",
        "  result_df = pd.DataFrame()\n",
        "  for i in np.arange(batch_size,len(data_org),batch_size):\n",
        "    #print(\"i= \",i)\n",
        "    #print(\"prev= \",prev)\n",
        "    data_batch = data_org.iloc[prev:i,:]\n",
        "    prev = i\n",
        "\n",
        "    question2 = data_batch['question']\n",
        "    question1 = pd.Series([query_question] * len(question2))\n",
        "    \n",
        "    #print(\"question1 = \",question1.shape)\n",
        "    #print(\"question2 = \",question2.shape)\n",
        "\n",
        "    results = predict_similarity_batch(question1, question2)\n",
        "    results = list(results.reshape(batch_size))\n",
        "    #print(\"results = \",len(results))\n",
        "    df = pd.DataFrame({'question1':list(question1), 'question2':list(question2),'scores':results})\n",
        "    df = df[df['scores']>=0.5]\n",
        "    result_df = pd.concat([df,result_df],axis = 0)\n",
        "    result_len = len(result_df)\n",
        "    print(result_len)\n",
        "    if result_len >= result_size:\n",
        "      break;\n",
        "  result_df = result_df.sort_values(by = 'scores',ascending = False)\n",
        "  result_df.columns = ['Query Question','Similar Questions','Scores']\n",
        "  result_df = result_df['Similar Questions'].reset_index(drop = True)\n",
        "  \n",
        "  return pd.Series(result_df)\n",
        "\n",
        "\n",
        "def generate_similar_questions(input_text,data):\n",
        "    question_limit = 3\n",
        "    questions = data['question']\n",
        "    cnt = 0\n",
        "    temp_list = []\n",
        "    for question in questions:\n",
        "      prediction = predict_similarity(input_text,question)\n",
        "      if (prediction >= 0.5) & (cnt <= 3):\n",
        "        cnt += 1\n",
        "        temp_list.append(question)\n",
        "      elif cnt > 3:\n",
        "        break;\n",
        "\n",
        "    \n",
        "    similar_questions_df = pd.DataFrame({'Similar Questions':temp_list})\n",
        "    return similar_questions_df\n",
        "\n",
        "\n",
        "# Define the sidebar navigation\n",
        "st.sidebar.title('Navigation')\n",
        "page = st.sidebar.selectbox('Select a page:', ['Question Similarity Checker', 'Similar Questions Generator'])\n",
        "\n",
        "# Define the pages\n",
        "if page == 'Question Similarity Checker':\n",
        "    # Set the app title\n",
        "    st.title('Question Similarity Checker')\n",
        "\n",
        "    # Define the input fields for the questions\n",
        "    question1 = st.text_area('Enter the first question:')\n",
        "    question2 = st.text_area('Enter the second question:')\n",
        "\n",
        "    # Define the submit button\n",
        "    submit_button = st.button('Submit')\n",
        "\n",
        "    # Check if the submit button is clicked\n",
        "    if submit_button:\n",
        "        try:\n",
        "            # Check if the questions are similar or not\n",
        "            if question1 and question2:\n",
        "                similarity_score = predict_similarity(question1, question2)\n",
        "                if similarity_score >= 0.5:\n",
        "                    #st.write('similiarity Score: ',similarity_score)\n",
        "                    st.success('The questions are similar')\n",
        "                else:\n",
        "                    #st.write('similiarity Score: ',similarity_score)\n",
        "                    st.warning('The questions are not similar')\n",
        "        except Exception as e:\n",
        "            st.error('An error occurred while checking the similarity: {}'.format(str(e)))\n",
        "\n",
        "elif page == 'Similar Questions Generator':\n",
        "    # Set the app title\n",
        "    st.title('Similar Questions Generator')\n",
        "\n",
        "    # Define the input field for the text\n",
        "    input_text = st.text_area('Enter some text:')\n",
        "\n",
        "    # Define the submit button\n",
        "    submit_button = st.button('Submit')\n",
        "\n",
        "    # Check if the submit button is clicked\n",
        "    if submit_button:\n",
        "        try:\n",
        "            # Generate similar questions\n",
        "            similar_questions_df = get_similar_questions(input_text,data_org)\n",
        "\n",
        "            # Display the result data frame\n",
        "            st.write(similar_questions_df)\n",
        "        except Exception as e:\n",
        "            st.error('An error occurred while generating similar questions: {}'.format(str(e)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykEWfV584Dyc",
        "outputId": "802ce322-c9bd-4ea7-bfd4-da19bd66b86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "AARlGT8fEMg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6458e13-c085-4e78-f5df-baaa55d6907d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.434s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9q-N5eKEPUf",
        "outputId": "bff64bde-16fd-43d8-ea43-393e5da63881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.871s\n",
            "your url is: https://neat-breads-appear-35-227-96-52.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBPQbN8J62c5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}